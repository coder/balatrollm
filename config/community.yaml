################################################################################
# Example configuration file for community leaderboard of balatrobench
# Usage: balatrollm config/community.yaml
################################################################################

################################################################################
# EXECUTION
################################################################################

parallel: 1

################################################################################
# GAME PARAMETERS
################################################################################

seed:
  - AAAAAAA
  - BBBBBBB
  - CCCCCCC
  - DDDDDDD
  - EEEEEEE
  - AAAAAAA # duplicate for multiple runs of same seed
  - BBBBBBB
  - CCCCCCC
  - DDDDDDD
  - EEEEEEE
  - AAAAAAA
  - BBBBBBB
  - CCCCCCC
  - DDDDDDD
  - EEEEEEE

deck:
  - RED

stake:
  - WHITE

################################################################################
# STRATEGY
################################################################################

# NOTE: here you should replace default with the actual name of your strategy
# Strategy name(s) from `src/balatrollm/strategies/`.

strategy:
  - default

################################################################################
# MODEL
################################################################################

# LLM model identifier(s). Format depends on your provider:
#   - OpenAI: "gpt-4o", "gpt-4o-mini"
#   - OpenRouter: "openai/gpt-4o", "anthropic/claude-3.5-sonnet"
# Requires BALATROLLM_BASE_URL and BALATROLLM_API_KEY env vars.

# NOTE: on the community leaderboard we going to use gpt-oss-20b. This is a
# pretty small LLM that can be run on consumer hardware with opensource
# inference engine (e.g. llama.cpp, LM Studio, Ollama, ...). It's suggested
# to optimize your strategy for this model.
#
# NOTE: For the official community leaderboard, models with at least 32k
# total context window are recommended to ensure the entire strategy, reasoning,
# and tool calls fit within the LLM context window. The leaderboard runs with
# the provider's default temperature (typically 1.0).

model:
  - openai/gpt-oss-20b

# NOTE: on the community leaderboard we run with `medium` reasoning effort

# Optional OpenAI client parameters passed to chat completions.
# Common options: seed, temperature, max_tokens, parallel_tool_calls
# The `extra_body` field passes provider-specific parameters (e.g., OpenRouter).
model_config:
  extra_body:
    usage:
      include: true
    reasoning:
      effort: medium
    provider:
      sort: throughput
################################################################################
# CONNECTION (optional - usually set via environment variables)
################################################################################

# BalatroBot JSON-RPC server host (default: 127.0.0.1)
# host: 127.0.0.1

# BalatroBot starting port (default: 12346)
# For parallel > 1, each worker uses port, port+1, port+2, etc.
# port: 12346

# LLM API base URL (prefer env var BALATROLLM_BASE_URL)
# base_url: https://openrouter.ai/api/v1

# LLM API key (prefer env var BALATROLLM_API_KEY) if your inference engine
# requires auth
# api_key: sk-...
